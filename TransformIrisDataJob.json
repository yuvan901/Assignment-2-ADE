{
	"jobConfig": {
		"name": "TransformIrisDataJob",
		"description": "",
		"role": "arn:aws:iam::891475953757:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "TransformIrisDataJob.py",
		"scriptLocation": "s3://aws-glue-assets-891475953757-ap-south-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-07-20T14:49:47.763Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-891475953757-ap-south-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-891475953757-ap-south-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# Get job arguments\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\n# Initialize Spark and Glue contexts\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\n\n# Read the Iris dataset from the provided file (uploaded to S3)\ninput_file_path = \"s3://assignment2irisspeciesdataset/Iris.csv/\"\n\n# Create a DynamicFrame using the Glue context\ndatasource0 = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\",\n    connection_options={\"paths\": [input_file_path]},\n    format=\"csv\",\n    format_options={\"withHeader\": True, \"separator\": \",\"}\n)\n\n# Apply mappings to transform data according to the actual columns in the dataset\napplymapping1 = ApplyMapping.apply(\n    frame=datasource0, \n    mappings=[\n        (\"Id\", \"double\", \"Id\", \"double\"),\n        (\"SepalLengthCm\", \"double\", \"SepalLengthCm\", \"double\"),\n        (\"SepalWidthCm\", \"double\", \"SepalWidthCm\", \"double\"),\n        (\"PetalLengthCm\", \"double\", \"PetalLengthCm\", \"double\"),\n        (\"PetalWidthCm\", \"double\", \"PetalWidthCm\", \"double\"),\n        (\"Species\", \"string\", \"Species\", \"string\")\n    ]\n)\n\n# Write transformed data to S3 in CSV format\noutput_path = \"s3://assignment2irisspeciesdataset/cleansed-iris-dataNew/\"\ndatasink2 = glueContext.write_dynamic_frame.from_options(\n    frame=applymapping1,\n    connection_type=\"s3\",\n    connection_options={\"path\": output_path, \"partitionKeys\": []},\n    format=\"csv\",\n    format_options={\"separator\": \",\"}\n)\n\n# Commit the job\njob.commit()\n\n"
}